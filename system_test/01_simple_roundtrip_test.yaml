- name: coyote
  title: kafka-backup

- name: Setup Cluster to Backup
  entries:
    - name: Docker Compose Up
      command: docker-compose up -d
    - name: Wait for Connect to get up
      command: >
        bash -c '
          echo "Trying to reach Kafka Connect. Try "
          for ((i=0;i<60;i++)); do
            docker-compose exec -T to-backup-connect curl "http://localhost:8083/connectors" && docker-compose exec -T restore-to-connect curl "http://localhost:8083/connectors" && break;
            echo "$i/60"
            sleep 10;
          done'

- name: Create  Topic for tests
  entries:
    - command: docker-compose exec -T to-backup-kafka runutil
          create_topic backup-test-1partition 1
- name: Produce Messages
  entries:
    - name: Produce 300 messages
      command: docker-compose exec -T to-backup-kafka runutil
          produce_messages backup-test-1partition 0 0 300
- name: Consume messages
  entries:
    - name: Consume 100 messages with cg-100
      command: docker-compose exec -T to-backup-kafka runutil
          consume_messages backup-test-1partition cg-100 100
    - name: Consume 200 messages with cg-200
      command: docker-compose exec -T to-backup-kafka runutil
          consume_messages backup-test-1partition cg-200 200
    - name: Consume 300 messages with cg-300
      command: docker-compose exec -T to-backup-kafka runutil
          consume_messages backup-test-1partition cg-300 300
- name: Check Consumer Group Offsets
  entries:
    - name: Check Consumer Group cg-100
      command: docker-compose exec -T to-backup-kafka runutil
          kafka_group_describe cg-100
      stdout_has: [ 'backup-test-1partition 0          100             300             200' ]
    - name: Check Consumer Group cg-200
      command: docker-compose exec -T to-backup-kafka runutil
          kafka_group_describe cg-200
      stdout_has: [ 'backup-test-1partition 0          200             300             100' ]
    - name: Check Consumer Group cg-200
      command: docker-compose exec -T to-backup-kafka runutil
          kafka_group_describe cg-300
      stdout_has: [ 'backup-test-1partition 0          300             300             0' ]

- name: Start Kafka Backup
  entries:
    - name: Clean previous data
      command: docker-compose exec -T to-backup-kafka runutil
          rm -rf "/kafka-backup/001_simple_1partition_test/"
    - name: Create an Kafka Backup Connector
      command: >
        docker-compose exec -T to-backup-connect
          curl -vs --stderr - -X POST -H "Content-Type: application/json"
               --data @-
               "http://localhost:8083/connectors"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
      stdin: |
        {
          "name": "backup-sink",
          "config": {
            "connector.class": "de.azapps.kafkabackup.sink.BackupSinkConnector",
            "tasks.max": "1",
            "topics.regex": "backup-test.*",
            "key.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "value.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "target.dir": "/kafka-backup/001_simple_1partition_test/",
            "max.segment.size.bytes": 10485760,
            "cluster.bootstrap.servers": "to-backup-kafka:9092"
          }
        }
    - command: sleep 30
      nolog: true

- name: Stop Cluster that was backed up
  entries:
    - name: Docker Compose Down
      command: docker-compose stop to-backup-kafka

- name: Restore
  entries:
    - name: Create Topic
      command: docker-compose exec -T restore-to-kafka runutil
          create_topic backup-test-1partition 1
    - name: Create an Kafka Backup Restore Connector
      command: >
        docker-compose exec -T restore-to-connect
          curl -vs --stderr - -X POST -H "Content-Type: application/json"
               --data @-
               "http://localhost:8083/connectors"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
      stdin: |
        {
          "name": "backup-source",
          "config": {
            "connector.class": "de.azapps.kafkabackup.source.BackupSourceConnector",
            "tasks.max": "1",
            "topics": "backup-test-1partition",
            "key.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "value.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "source.dir": "/kafka-backup/001_simple_1partition_test/",
            "batch.size": 1000,
            "cluster.bootstrap.servers": "restore-to-kafka:9092",
            "cluster.key.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer",
            "cluster.value.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer"
          }
        }
    - command: sleep 30
      nolog: true

- name: Verify Backup
  entries:
    - name: Verify Records
      command: docker-compose exec -T restore-to-kafka runutil
          consume_verify_messages backup-test-1partition 0 300
    - name: Check Consumer Group cg-100
      command: docker-compose exec -T restore-to-kafka runutil
          kafka_group_describe cg-100
      stdout_has: [ 'backup-test-1partition 0          100             300             200' ]
    - name: Check Consumer Group cg-200
      command: docker-compose exec -T restore-to-kafka runutil
          kafka_group_describe cg-200
      stdout_has: [ 'backup-test-1partition 0          200             300             100' ]
    - name: Check Consumer Group cg-200
      command: docker-compose exec -T restore-to-kafka runutil
          kafka_group_describe cg-300
      stdout_has: [ 'backup-test-1partition 0          300             300             0' ]

- name: Clean-up Containers
  entries:
    - name: Docker Compose Down
      command: docker-compose down
