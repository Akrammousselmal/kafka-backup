# * Create a backup with multiple segments
# * Delete some old segments
# * Delete all indexes
# * Recreate all indexes
# * Do a restore
- name: coyote
  title: kafka-backup

- name: Setup Cluster to Backup
  entries:
    - name: Docker Compose Up
      command: docker-compose up -d
    - name: Wait for Connect to get up
      command: >
        bash -c '
          echo "Trying to reach Kafka Connect. Try "
          for ((i=0;i<60;i++)); do
            docker-compose exec -T to-backup-connect curl "http://localhost:8083/connectors" && docker-compose exec -T restore-to-connect curl "http://localhost:8083/connectors" && break;
            echo "$i/60"
            sleep 10;
          done'
    - name: Clean previous data
      command: docker-compose exec -T to-backup-kafka runutil
        rm -rf "/kafka-backup/04_delete_old_segment/"

- name: Create  Topic for tests
  entries:
    - command: docker-compose exec -T to-backup-kafka runutil
        create_topic backup-test-1partition 1
- name: Produce Messages
  entries:
    - name: Produce 3000 messages
      command: docker-compose exec -T to-backup-kafka runutil
        produce_messages backup-test-1partition 0 0 3000

- name: Start Kafka Backup
  entries:
    - name: Create an Kafka Backup Connector
      command: >
        docker-compose exec -T to-backup-connect
          curl -vs --stderr - -X POST -H "Content-Type: application/json"
               --data @-
               "http://localhost:8083/connectors"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
      stdin: |
        {
          "name": "backup-sink",
          "config": {
            "connector.class": "de.azapps.kafkabackup.sink.BackupSinkConnector",
            "tasks.max": "1",
            "topics.regex": "backup-test.*",
            "key.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "value.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "target.dir": "/kafka-backup/04_delete_old_segment/",
            "max.segment.size.bytes": 10485760,
            "cluster.bootstrap.servers": "to-backup-kafka:9092"
          }
        }
    - command: sleep 30
      nolog: true

- name: Stop Cluster that was backed up
  entries:
    - name: Docker Compose Down
      command: docker-compose stop to-backup-kafka

- name: Delete old segment and restore the index
  entries:
    - name: Delete all indexes
      command: docker-compose exec -T restore-to-connect bash -c  \
        'rm /kafka-backup/04_delete_old_segment/backup-test-1partition/*index*'
    - name: Delete old segment
      command: docker-compose exec -T restore-to-connect bash -c  \
        'rm /kafka-backup/04_delete_old_segment/backup-test-1partition/segment_partition_000_from_offset_0000000000_records'
    - name: Restore segment and partition indexes
      command: >
        docker-compose exec -T restore-to-connect bash -c  '
        export TOPICDIR="/kafka-backup/04_delete_old_segment/backup-test-1partition/" &&
        export CLASSPATH="/connect-plugins/kafka-backup.jar" &&
        for f in "$TOPICDIR"/segment_partition_*_records ; do
          java de.azapps.kafkabackup.cli.SegmentIndexCLI --restore-index \
          --segment $f
        done &&
        java de.azapps.kafkabackup.cli.PartitionIndexCLI --restore --partition 0 --topic-dir "$TOPICDIR"'

- name: Restore
  entries:
    - name: Create Topic
      command: docker-compose exec -T restore-to-kafka runutil
        create_topic backup-test-1partition 1
    - name: Create an Kafka Backup Restore Connector
      command: >
        docker-compose exec -T restore-to-connect
          curl -vs --stderr - -X POST -H "Content-Type: application/json"
               --data @-
               "http://localhost:8083/connectors"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
      stdin: |
        {
          "name": "backup-source",
          "config": {
            "connector.class": "de.azapps.kafkabackup.source.BackupSourceConnector",
            "tasks.max": "1",
            "topics": "backup-test-1partition",
            "key.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "value.converter": "de.azapps.kafkabackup.common.AlreadyBytesConverter",
            "source.dir": "/kafka-backup/04_delete_old_segment/",
            "batch.size": 1000,
            "cluster.bootstrap.servers": "restore-to-kafka:9092",
            "cluster.key.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer",
            "cluster.value.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer"
          }
        }
    - command: sleep 3
      nolog: true
    - name: Check For errors
      timeout: 300s
      command: docker-compose exec -T restore-to-connect curl -vs "http://localhost:8083/connectors/backup-source/status"
      stderr_has: ["200 OK"]
      stdout_has: ["RUNNING"]
      stdout_not_has: ["FAILED"]
    - command: sleep 30
      nolog: true

- name: Verify Backup
  entries:
    - name: Verify Records
      command: docker-compose exec -T restore-to-kafka runutil
        consume_verify_messages backup-test-1partition 0 1041 300

#- name: Clean-up Containers
#  entries:
#    - name: Docker Compose Down
#      command: docker-compose down